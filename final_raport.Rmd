---
title: "RR_presentation"
date: "2024-05-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load the libraries in a reproducible way.
```{r libraries, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# Libraries
if (!require(dplyr)) {install.packages("dplyr"); library(dplyr)}
if (!require(ggplot2)) {install.packages("ggplot2"); library(ggplot2)}
if (!require(plm)) {install.packages("plm"); library(plm)}
if (!require(zoo)) {install.packages("zoo"); library(zoo)}
if (!require(lmtest)) {install.packages("lmtest"); library(lmtest)}
```
```{r read}
# Read the data
data <- read.csv(file = "preprocessed_data.csv")
data <- data %>% select(-X)

# Check the structure
str(data)
```

We perform Exploratory Data Analysis to get to know the data.
```{r EDA}
# Summary statistics
data %>%
  select(-c(year_month, time)) %>%
  group_by(country) %>%
  summarize(mean_sky = mean(sky), 
            mean_temperature = mean(temperature),
            sum_streams = sum(streams)) %>%
  arrange(desc(sum_streams))
```

```{r dist}
# Check the distribution of variables

# Write a function "plot_histogram" to plot the distribution of every numeric variable

plot_histograms <- function(data) {
  
  numeric_cols <- sapply(data, is.numeric)
  data_numeric <- data[, numeric_cols] %>% select(-c('time', 'year_month'))
  
  for (col in colnames(data_numeric)) {
    hist(data_numeric[[col]], main = paste("Histogram of", col), xlab = col, ylab = "Frequency", col = "blue", border = "black")
  }
}


plot_histograms(data)
```

Based on the distribution of the variables we apply the logarithmic transformation
to symmetrize the distribution.
```{r feature}
# Feature engineering

# Apply log(x + 1) transformation for variables with highly non symmetric distribution
par(mfrow = c(1,2))
hist(log1p(data$streams), main = "Histogram of variable log_streams", col = "blue", border = "black")
hist(data$streams, main = "Histogram of variable streams", col = "blue", border = "black")
par(mfrow = c(1,1))

data$log_streams <- log1p(data$streams)

par(mfrow = c(1,2))
hist(data$af_acousticness, main = "Histogram of variable log_af_acousticness",
     col = "tomato", border = "black")
hist(log1p(data$af_acousticness), main = "Histogram of variable af_acousticness",
     col = "tomato", border = "black")
par(mfrow = c(1,1))

data$log_af_acousticness <- log1p(data$af_acousticness)
```
Initially we will calculate Fixed Effects Model, Random Effects Model and
simple Pooled Regression. Then we will decide which model is most appropriate
regarding to the data we have.
```{r FE}
# Modelling

# Fixed Effects Model
model.fixed <- plm(af_valence ~ sky + temperature + log_streams + 
                     af_danceability + af_energy + af_key + 
                     af_loudness + af_speechiness + af_acousticness + 
                     af_tempo, data = data, index = c("country", "year_month"),
                   model = "within")

summary(model.fixed)
```

```{r RE}
# Random Effects Model
model.random <- plm(af_valence ~ sky + temperature + log_streams + 
                     af_danceability + af_energy + af_key + 
                     af_loudness + af_speechiness + af_acousticness + 
                     af_tempo, data = data, index = c("country", "year_month"),
                   model = "random")

summary(model.random)
```

```{r PLR}
# Pooled Regression Model
model.OLS <- lm(af_valence ~ sky + temperature + log_streams + 
                     af_danceability + af_energy + af_key + 
                     af_loudness + af_speechiness + af_acousticness + 
                     af_tempo, data = data)

summary(model.OLS)
```

We perform Hausman test to check which model to choose between Fixed Effects Model
and Random Effects Model.  
Null hypothesis: Cov(u,X) = 0. 
Alternative hypothesis: Cov(u,X) != 0. 
According to the results of the test we go for Fixed Effects Model.

```{r hausman}
# Select between Fixed Effects and Random Effects model

# Hausman test
phtest(model.fixed, model.random)
# p-value < 5% (significance level alpha), we reject null hypothesis in favor
# of alternative hypothesis - Cov(u,X) != 0 - only Fixed Effects Model is appropriate
```
As a next step we perform pFtest for poolability.  
Null hypothesis: Individual effects are jointly insignificant. 
Alternative hypothesis: Indifivual effects are jointly significant.  
According to the results of the test we go for Fixed Effects Model.

```{r pFtest}
# Test for poolability
pFtest(model.fixed, model.OLS)
# p-value < 5% (significance level alpha), we reject null hypothesis
# in favor of alternative hypothesis - fixed effects model is more appropriate
```

As the model is selected, we check the assumptions - lack of autocorrelation and
homoscedasticity of the residuals. We will perform respectively Breusch-Godfrey
test and Breusch-Pagan test.
  
Breusch-Pagan test null hypothesis: Serial correlation does not exist in indiosyncratic
errors, alternative hypothesis: Serial correlation exists in indiosyncratic errors.
According to the result of the test, our model exhibit serial correlation in the
indiosyncratic errors.
```{r bgtest}
# Test for correlation (Breusch-Godfrey test)
pbgtest(model.fixed)
# p-value < 5% (significance level alpha), we reject null hypothesis
# in favor of alternative hypothbesis - serial correlation exists in 
# indiosyncratic errors
```
On the other hand due to the Breusch-Pagan test, the residuals in our model
are heteroscedastic. Therefore we cannot conclude on the standard errors in the model,
they are biased. To prevent from that we will use robust standard errors.

```{r bptest}
# Test for heteroscedasticity (Breusch-Pagan test)
bptest(af_valence ~ sky + temperature + log_streams + 
                  af_danceability + af_energy + af_key + 
                  af_loudness + af_speechiness + af_acousticness + 
                  af_tempo, 
       data = data,
       studentize = T)
# p-value < 5% (significance level alpha), we reject null hypothesis
# in favor of alternative hypothesis - errors in the model are heteroscedastic

```
As the conditions about lack of autocorrelation of the errors and homoscedasticity of the errors
are not met, we applied robust standard errors. Now we can conclude on the significance of the
variables. Standard errors are not biased.

```{r final_model}
# Apply robust standard errors to obtain unbiased estimations
final_model <- coeftest(model.fixed, vcov. = vcovHC(model.fixed, 
                                     method = "white1",
                                     type = "HC0",
                                     cluster = "group"))

final_model

```
The regression results indicate that several predictors have significant effects on the dependent variable. Temperature has a positive and highly significant impact, with a unit increase in temperature leading to an approximate 0.000358 unit increase in the dependent variable, as indicated by the very low p-value of 2.546e-06. Similarly, log_streams positively affects the dependent variable, where a one-unit increase corresponds to a 0.016341 unit increase, supported by a p-value of 0.0003969.

Danceability shows a negative and significant relationship with the dependent variable, where an increase by one unit results in a 0.19860 unit decrease. This is substantiated by the p-value of 0.0059365. Energy has a positive and significant effect, with a unit increase leading to a 0.24493 unit increase in the dependent variable, and a p-value of 0.0248897. Acousticness also positively and significantly affects the dependent variable, where a one-unit increase corresponds to a 0.15533 unit increase, as indicated by the p-value of 0.0070760. Tempo has a positive and significant relationship with the dependent variable, with a one-unit increase resulting in a 0.0013468 unit increase, supported by a p-value of 0.0066370.

On the other hand, sky, key, loudness, and speechiness do not show significant effects on the dependent variable. Sky has a negative but non-significant effect, with a high p-value of 0.7431509. Key has a positive but non-significant effect, supported by a p-value of 0.7011131. Loudness also shows a positive but non-significant relationship, with a p-value of 0.5366720. Lastly, speechiness, despite being positive, does not significantly affect the dependent variable, as indicated by the p-value of 0.4118580.
