---
title: "RR_presentation"
date: "2024-05-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load the libraries in a reproducible way.
```{r libraries, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# Libraries
if (!require(dplyr)) {install.packages("dplyr"); library(dplyr)}
if (!require(ggplot2)) {install.packages("ggplot2"); library(ggplot2)}
if (!require(plm)) {install.packages("plm"); library(plm)}
if (!require(zoo)) {install.packages("zoo"); library(zoo)}
if (!require(lmtest)) {install.packages("lmtest"); library(lmtest)}
```
```{r read}
# Read the data
data <- read.csv(file = "preprocessed_data.csv")
data <- data %>% select(-X)

# Check the structure
str(data)
```

We perform Exploratory Data Analysis to get to know the data.
```{r EDA}
# Summary statistics
data %>%
  select(-c(year_month, time)) %>%
  group_by(country) %>%
  summarize(mean_sky = mean(sky), 
            mean_temperature = mean(temperature),
            sum_streams = sum(streams)) %>%
  arrange(desc(sum_streams))
```


```{r dist}
# Check the distribution of variables

# Write a function "plot_histogram" to plot the distribution of every numeric variable

plot_histograms <- function(data) {
  
  numeric_cols <- sapply(data, is.numeric)
  data_numeric <- data[, numeric_cols]
  for (col in colnames(data_numeric)) {
    hist(data_numeric[[col]], main = paste("Histogram of", col), xlab = col, ylab = "Frequency", col = "blue", border = "black")
  }
}


plot_histograms(data)
```

Based on the distribution of the variables we apply the logarithmic transformation
to symmetrize the distribution.
```{r feature}
# Feature engineering

# Apply log(x + 1) transformation for variables with highly non symmetric distribution
par(mfrow = c(1,2))
hist(log1p(data$streams), main = "Histogram of variable log_streams", col = "blue", border = "black")
hist(data$streams, main = "Histogram of variable streams", col = "blue", border = "black")
par(mfrow = c(1,1))

data$log_streams <- log1p(data$streams)

par(mfrow = c(1,2))
hist(data$af_acousticness, main = "Histogram of variable log_af_acousticness",
     col = "tomato", border = "black")
hist(log1p(data$af_acousticness), main = "Histogram of variable af_acousticness",
     col = "tomato", border = "black")
par(mfrow = c(1,1))

data$log_af_acousticness <- log1p(data$af_acousticness)
```
Initially we will calculate Fixed Effects Model, Random Effects Model and
simple Pooled Regression. Then we will decide which model is most appropriate
regarding to the data we have.
```{r FE}
# Modelling

# Fixed Effects Model
model.fixed <- plm(af_valence ~ sky + temperature + log_streams + 
                     af_danceability + af_energy + af_key + 
                     af_loudness + af_speechiness + af_acousticness + 
                     af_tempo, data = data, index = c("country", "year_month"),
                   model = "within")

summary(model.fixed)
```

```{r RE}
# Random Effects Model
model.random <- plm(af_valence ~ sky + temperature + log_streams + 
                     af_danceability + af_energy + af_key + 
                     af_loudness + af_speechiness + af_acousticness + 
                     af_tempo, data = data, index = c("country", "year_month"),
                   model = "random")

summary(model.random)
```

```{r PLR}
# Pooled Regression Model
model.OLS <- lm(af_valence ~ sky + temperature + log_streams + 
                     af_danceability + af_energy + af_key + 
                     af_loudness + af_speechiness + af_acousticness + 
                     af_tempo, data = data)

summary(model.OLS)
```

We perform Hausman test to check which model to choose between Fixed Effects Model
and Random Effects Model.  
Null hypothesis: Cov(u,X) = 0. 
Alternative hypothesis: Cov(u,X) != 0. 
According to the results of the test we go for Fixed Effects Model.

```{r hausman}
# Select between Fixed Effects and Random Effects model

# Hausman test
phtest(model.fixed, model.random)
# p-value < 5% (significance level alpha), we reject null hypothesis in favor
# of alternative hypothesis - Cov(u,X) != 0 - only Fixed Effects Model is appropriate
```
As a next step we perform pFtest for poolability.  
Null hypothesis: Individual effects are jointly insignificant. 
Alternative hypothesis: Indifivual effects are jointly significant.  
According to the results of the test we go for Fixed Effects Model.

```{r pFtest}
# Test for poolability
pFtest(model.fixed, model.OLS)
# p-value < 5% (significance level alpha), we reject null hypothesis
# in favor of alternative hypothesis - fixed effects model is more appropriate
```

As the model is selected, we check the assumptions - lack of autocorrelation and
homoscedasticity of the residuals. We will perform respectively Breusch-Godfrey
test and Breusch-Pagan test.
  
Breusch-Pagan test null hypothesis: Serial correlation does not exist in indiosyncratic
errors, alternative hypothesis: Serial correlation exists in indiosyncratic errors.
According to the result of the test, our model exhibit serial correlation in the
indiosyncratic errors.
```{r bgtest}
# Test for correlation (Breusch-Godfrey test)
pbgtest(model.fixed)
# p-value < 5% (significance level alpha), we reject null hypothesis
# in favor of alternative hypothbesis - serial correlation exists in 
# indiosyncratic errors
```
On the other hand due to the Breusch-Pagan test, the residuals in our model
are heteroscedastic. Therefore we cannot conclude on the standard errors in the model,
they are biased. To prevent from that we will use robust standard errors.

```{r bptest}
# Test for heteroscedasticity (Breusch-Pagan test)
bptest(af_valence ~ sky + temperature + log_streams + 
                  af_danceability + af_energy + af_key + 
                  af_loudness + af_speechiness + af_acousticness + 
                  af_tempo, 
       data = data,
       studentize = T)
# p-value < 5% (significance level alpha), we reject null hypothesis
# in favor of alternative hypothesis - errors in the model are heteroscedastic

```
As the conditions about lack of autocorrelation of the errors and homoscedasticity of the errors
are not met, we applied robust standard errors. Now we can conclude on the significance of the
variables. Standard errors are not biased.

```{r final_model}
# Apply robust standard errors to obtain unbiased estimations
final_model <- coeftest(model.fixed, vcov. = vcovHC(model.fixed, 
                                     method = "white1",
                                     type = "HC0",
                                     cluster = "group"))

final_model

```