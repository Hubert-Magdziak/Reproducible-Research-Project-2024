{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of countries names and country codes\n",
    "\n",
    "country_code_to_name = {\n",
    "    \"at\": \"Austria\", \"be\": \"Belgium\", \"ca\": \"Canada\", \"ch\": \"Switzerland\",\n",
    "    \"cz\": \"Czech Republic\", \"de\": \"Germany\", \"dk\": \"Denmark\", \"es\": \"Spain\",\n",
    "    \"fi\": \"Finland\", \"fr\": \"France\", \"gb\": \"United Kingdom\", \"gr\": \"Greece\",\n",
    "    \"hu\": \"Hungary\", \"ie\": \"Ireland\", \"is\": \"Iceland\", \"it\": \"Italy\",\n",
    "    \"jp\": \"Japan\", \"mx\": \"Mexico\", \"nl\": \"Netherlands\", \"no\": \"Norway\",\n",
    "    \"pl\": \"Poland\", \"pt\": \"Portugal\", \"se\": \"Sweden\", \"sk\": \"Slovakia\",\n",
    "    \"tr\": \"Turkey\", \"us\": \"United States\"\n",
    "}\n",
    "\n",
    "country_codes = list(country_code_to_name.keys())\n",
    "\n",
    "country_names = list(country_code_to_name.values())\n",
    "\n",
    "# columns from original dataset which we find useful for our analysis, due to the size of data we will keep as little data as possible\n",
    "columns_to_keep = [\n",
    "    'date', 'region', 'chart', 'streams', 'af_danceability',\n",
    "    'af_energy', 'af_key', 'af_loudness', 'af_speechiness', \n",
    "    'af_acousticness', 'af_valence', 'af_tempo'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'read_big_file' and general approach to working on big .csv files was desgined based on the article of Ankush Kunwar from Medium, the link might be found below:\n",
    "\n",
    "https://ankushkunwar7777.medium.com/working-with-large-csv-file-using-python-1ec6577c5ce6\n",
    "\n",
    "The 'preprocessing' function itself was created to match our needs during the process of preprocessing. The data is filtered by the date, region, type of charts from which it was aggregated and lastly only the columns which we wanted to keep were selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data_frame):\n",
    "    data_frame['date'] = pd.to_datetime(data_frame['date'])\n",
    "    \n",
    "    # filter by date\n",
    "    date_filter = (data_frame['date'] >= '2018-01-01') & (data_frame['date'] <= '2019-12-31')\n",
    "\n",
    "    # filter by countries names\n",
    "    region_filter = data_frame['region'].isin(country_names)\n",
    "    \n",
    "    data_frame = data_frame[date_filter & region_filter]\n",
    "\n",
    "    # selection of wanted columns\n",
    "    data_frame = data_frame[columns_to_keep]\n",
    "\n",
    "    # filter by chart type\n",
    "    data_frame = data_frame[data_frame['chart'] == 'top200']\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "def read_big_file(file_name):\n",
    "\n",
    "    for code_chunk in pd.read_csv(file_name, chunksize = 10000):\n",
    "        yield code_chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below loop allows to create the 'processed_data.csv' which contains the data from original source after our preprocessing. The loop was upgraded from appending empty chunks to simply continuing to save processing time. After the first chunk, where the headers are written to the file, the data is simply appended to the existing file if the next chunk is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'processed_data.csv'\n",
    "\n",
    "header_written = False\n",
    "\n",
    "for data_frame in read_big_file('merged_data.csv'):\n",
    "    processed_data = preprocessing(data_frame)\n",
    "\n",
    "    if processed_data.empty:\n",
    "        continue\n",
    "\n",
    "    if not header_written:\n",
    "        processed_data.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        processed_data.to_csv(output_file, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = pd.read_csv('processed_data.csv')\n",
    "\n",
    "# qualitative check if we have the same number of countries - its OK, 26 countries\n",
    "len(processed_df['region'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final preparation of aggregated by month and country data. Dynamic dictionary comprehension is used so that we can only change contents of 'columns_to_average' if any modifications are needed.\n",
    "The final data frame is 'aggregated_monthly_data.csv' and it will be used in further analysis and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure that the dates are in correct format\n",
    "processed_df['date'] = pd.to_datetime(processed_df['date'])\n",
    "\n",
    "# changning date format so that it matches original data\n",
    "processed_df['year_month'] = processed_df['date'].dt.strftime('%Y%m')\n",
    "\n",
    "# chosen columns on which the aggregation will be averaged for the month and region\n",
    "columns_to_average = [\n",
    "    'af_danceability', 'af_energy', 'af_key', 'af_loudness', \n",
    "    'af_speechiness', 'af_acousticness', 'af_valence', 'af_tempo'\n",
    "]\n",
    "\n",
    "# group by 'region' and 'year_month' to calculate aggregate values\n",
    "# .reset_index to get region and year_month back to be columns, not multi-indexes\n",
    "aggregated_df = processed_df.groupby(['region', 'year_month']).agg({\n",
    "    'streams': 'sum',\n",
    "    **{col: 'mean' for col in columns_to_average}\n",
    "}).reset_index()\n",
    "\n",
    "aggregated_df.to_csv('aggregated_monthly_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
